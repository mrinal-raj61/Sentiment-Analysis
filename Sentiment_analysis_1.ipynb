{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment_analysis_1.ipynb",
      "provenance": [],
      "mount_file_id": "1QYfhMel8B7U8q4C84yrch2Rhby3xFuW9",
      "authorship_tag": "ABX9TyOjf1fKdSzNLElzQ3R9qHY+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mrinal-raj61/Sentiment-Analysis/blob/main/Sentiment_analysis_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHiBPYV9yXqa"
      },
      "source": [
        "import pandas as pd\n",
        "df_raw = pd.read_csv('review_10000.csv')\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tc-MlohOzSvn"
      },
      "source": [
        "#assigning score above 3 to 1 and below 3 to -1\n",
        "\n",
        "df_raw = df_raw[df_raw['Score'] != 4]\n",
        "df_raw['Sentiment'] = df_raw['Score'].apply(lambda rating :\n",
        "                                            +1 if rating > 4 else -1)\n",
        "y = df_raw['Sentiment']\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JtXnTAgi0BN9"
      },
      "source": [
        "dfNew = df_raw[['Summary','Sentiment']]\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XB4GK5zAhIlg"
      },
      "source": [
        "#only negative reveiw\n",
        "dfNew_neg = dfNew[dfNew['Sentiment'] == -1]\n",
        "\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2azGhoXaiVlj"
      },
      "source": [
        "#only positive reveiw and number of rows are same as negative reveiws\n",
        "dfNew_pos =  dfNew[dfNew['Sentiment'] == 1]\n",
        "\n",
        "dfNew_pos = dfNew_pos.head(len(dfNew_neg.index))\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4_8X4zFj7J1"
      },
      "source": [
        "#merge both positive and negative reveiws\n",
        "df = dfNew_pos.append(dfNew_neg)\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCibwBxW3kR4"
      },
      "source": [
        "DATA CLEANING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XfmWRw50XYB"
      },
      "source": [
        "def remove_punctuation(text):\n",
        "    final = \"\".join(u for u in text if u not in (\"?\", \".\", \";\", \":\",  \"!\",'\"',\",\",\"/\" ))\n",
        "    return final\n",
        "\n",
        "df = df.dropna(subset=['Summary'])\n",
        "df['Summary'] = df['Summary'].apply(remove_punctuation)\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzn4ORU906HT"
      },
      "source": [
        "dfNew2 = df[['Summary','Sentiment']]\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Mz1SuZ8B7Rf",
        "outputId": "31e676ca-bd06-40a5-ca82-00da0569f3e0"
      },
      "source": [
        "#remove stopwords and tokenize\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from keras.preprocessing.text import Tokenizer"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTardRK-4CY9"
      },
      "source": [
        "\n",
        "\n",
        "clean_text = []\n",
        "for col,row in dfNew2.iterrows():\n",
        "  text3 = (row[0])\n",
        "  text = text3.lower()\n",
        "  text_tokens = word_tokenize(text)\n",
        "  tokens_without_sw = [word for word in text_tokens if not word in stopwords.words()]\n",
        "  text = (\" \").join(tokens_without_sw)\n",
        "  clean_text.append(text)\n",
        "dfNew2['Statement'] = clean_text\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2PJKNcm0eQ0"
      },
      "source": [
        "# random split train and test data\n",
        "import numpy as np\n",
        "index = df.index\n",
        "df['random_number'] = np.random.randn(len(index))\n",
        "train = df[df['random_number'] <= 0.75]\n",
        "test = df[df['random_number'] > 0.75]\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dljKAux1xUH"
      },
      "source": [
        "# count vectorizer:\n",
        "#Create a bag of words\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vectorizer = CountVectorizer(token_pattern=r'\\b\\w+\\b')\n",
        "train_matrix = vectorizer.fit_transform(train['Summary'])\n",
        "test_matrix = vectorizer.transform(test['Summary'])\n",
        "\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAXebSWj18vl"
      },
      "source": [
        "# Logistic Regression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "lr = LogisticRegression()"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dM-1rxgD2HWE"
      },
      "source": [
        "#Split target and independent variables\n",
        "X_train = train_matrix\n",
        "X_test = test_matrix\n",
        "y_train = train['Sentiment']\n",
        "y_test = test['Sentiment']"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bzDG_AT-2Tub",
        "outputId": "88e11ed0-ff0e-4a5e-db02-db2a3751d891"
      },
      "source": [
        "lr.fit(X_train,y_train)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Qu1w1ZE25xV"
      },
      "source": [
        "predictions = lr.predict(X_test)\n",
        "\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "562RrODz2Z3U",
        "outputId": "7b6b35b1-d22c-4196-ca41-1a4a2de7073e"
      },
      "source": [
        "# find accuracy, precision, recall:\n",
        "from sklearn.metrics import confusion_matrix,classification_report\n",
        "new = np.asarray(y_test)\n",
        "confusion_matrix(predictions,y_test)\n",
        "\n",
        "\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[432, 118],\n",
              "       [ 57, 433]])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "haD6TTT24hV5"
      },
      "source": [
        "\n",
        "\n",
        "def classifier(text2):\n",
        "\n",
        " text = text2.lower()\n",
        " new1 = remove_punctuation(text)\n",
        " text_tokens = word_tokenize(new1)\n",
        " if (text_tokens.count(\"not\") >= 1 ):\n",
        "  a = text_tokens.count(\"not\")\n",
        "  fresh_token  = [word for word in text_tokens if not word in stopwords.words()]\n",
        "  #fresh_token.replace(\"not\",\" \")\n",
        "  fresh_token = [item.replace(\"not\", \" \") for item in fresh_token]\n",
        "  #print(fresh_token)\n",
        "  text = (\" \").join(fresh_token)\n",
        "  text = [text]\n",
        "  check_matrix = vectorizer.transform(text)\n",
        "  X_check = check_matrix\n",
        "  predicted_value = lr.predict(X_check)\n",
        "  p = predicted_value * -1**a\n",
        "  if (p == 1):\n",
        "    return(\"Positive Review\")\n",
        "  if (p == -1):\n",
        "    return(\"Negative Review\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " \n",
        " else:\n",
        "  fresh_token = [word for word in text_tokens if not word in stopwords.words() ]\n",
        "\n",
        "  text = (\" \").join(fresh_token)\n",
        "  text = [text]\n",
        "  check_matrix = vectorizer.transform(text)\n",
        "  X_check = check_matrix\n",
        "  predicted_value = lr.predict(X_check)\n",
        "  if (predicted_value == 1):\n",
        "    return(\"Positive Review\")\n",
        "  if (predicted_value == -1):\n",
        "    return(\"Negative Review\")\n",
        "\n",
        "  \n",
        "  \n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F6dDvCx_PAZ4",
        "outputId": "a997cd6d-0412-4648-d05e-4ff14c0c2045"
      },
      "source": [
        "new = ' this is very tasty. I liked it'\n",
        "print(classifier(new))\n"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Positive Review\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-LZPJ2lVL0t",
        "outputId": "6d616985-621e-46de-c6c2-6299999a1b7d"
      },
      "source": [
        "new = 'This is the worst food of my life'\n",
        "print(classifier(new))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Negative Review\n"
          ]
        }
      ]
    }
  ]
}